{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c161570-c320-4e4f-9f80-bb46f36325c3",
   "metadata": {},
   "source": [
    "***\n",
    "# **<center>COURS PYTHON 2IMACS #10</center>**\n",
    "# ***<center>Traitement d'images</center>***\n",
    "# ***<center>Open CV</center>***\n",
    "# ***<center>Scikit Image</center>***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7634a93-614b-483f-b905-42fd9195cbab",
   "metadata": {},
   "source": [
    "Plusieurs bibliothèques Python sont disponibles pour réaliser des traitements d'images. Nous allons notamment voir comment appliquer differents filtres , seuiller des images, détecter et mesurer des objets présents dans des images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a266a-d5a8-4705-bb90-f33acfc2d324",
   "metadata": {},
   "source": [
    "# 10-1 Bibliothèque OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97aa60-b599-43ab-9875-cd7d6b317d74",
   "metadata": {},
   "source": [
    "OpenCV pour Open Computer Vision est développées en C++, Python et Java, elle permet les opérations de base de traitement d'image (lecture écriture, sauvegarde, calcul d'histogrammes, seuillages...).\n",
    "Elle permet également de travailler sur des vidéos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e81bf-a253-4340-ad79-c089af42dc36",
   "metadata": {},
   "source": [
    "## 10-1-1 Ouverture et Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d55f40-f7f5-41f1-ae95-223bef8819af",
   "metadata": {},
   "source": [
    "### 10-1-1-1 Ouverture du fichier image et affichage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7ee88-5919-4e0b-bd2f-d0da8da9c54a",
   "metadata": {},
   "source": [
    "OpenCv peut ouvrir la plupart des format d'images courant(jpg, png, tif, bmp...). Aprés l'importation de la bibliothèque, affectons les données contenues dans le fichier tournesols.jpg à img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d683b39-0837-474b-9b85-ac6cd93c32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"fichiers_cours/traitement_image/tournesols.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede79db7-2611-46fc-936a-d1297169e118",
   "metadata": {},
   "source": [
    "Affichons l'image img et donnons lui un titre.   \n",
    "**Attention:** avec k = cv2.waitKey(0) on précise que l'on attend une action pour fermer l'image, il est conseillé de la fermer avec la x en haut à droite de la fenêtre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d8b44-603a-4d2a-ac62-b1f78efb9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Le titre pour le bandeau de mon image\",img)\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2853c-9f4d-4ab0-8e9a-cd8240a4b5e7",
   "metadata": {},
   "source": [
    "### 10-1-1-2 Structure de l'image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f076e7c-132a-4638-8d69-5b6bb733fe26",
   "metadata": {},
   "source": [
    "img est maintenant un tableau numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48c8d5-888d-4c1c-b095-87044a93c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03633ab-11eb-4998-8225-0ed4f0b7daa5",
   "metadata": {},
   "source": [
    "Visualisons ses dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910abe92-d6bd-4953-bdd0-435bbaa9fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('dimensions : ',img.shape)\n",
    "# Trouver les valeurs minimale et maximale des intensité des pixels avec numpy\n",
    "min_val = np.min(img)\n",
    "max_val = np.max(img)\n",
    "print('valeur mini pixel : ',min_val,'valeur maxi pixel : ', max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4afbe-528e-4f4c-80ee-3244fe5eadb1",
   "metadata": {},
   "source": [
    "On a bien une image couleur en 3 canaux de dimension 800x600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a45a4a-94b8-402d-87fc-7382bba59d17",
   "metadata": {},
   "source": [
    "Affichons son histogramme avec cv2.calcHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21042f-201b-44ce-867b-c13a1f55c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Décomposer les canaux de couleur\n",
    "blue_channel = img[:, :, 0]\n",
    "green_channel = img[:, :, 1]\n",
    "red_channel = img[:, :, 2]\n",
    "\n",
    "# Définir un masque, c'est à dire une zone de l'image sur laquelle calculer \n",
    "#l'histogramme (peut être None si vous ne voulez pas utiliser de masque)\n",
    "mask = None\n",
    "\n",
    "# Définir le nombre de bins (256 pour chaque canal)\n",
    "bins = [256]\n",
    "\n",
    "# Définir la plage des valeurs de pixel (0 à 256 pour chaque canal)\n",
    "pixel_range = [0, 256]\n",
    "\n",
    "# Calculer l'histogramme pour chaque canal de couleur avec un masque\n",
    "hist_blue = cv2.calcHist([blue_channel],[0], mask, bins, pixel_range)\n",
    "hist_green = cv2.calcHist([green_channel], [0], mask, bins, pixel_range)\n",
    "hist_red = cv2.calcHist([red_channel], [0], mask, bins, pixel_range)\n",
    "\n",
    "\n",
    "\n",
    "# Afficher les histogrammes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hist_blue, color='blue', label='Blue Channel')\n",
    "plt.plot(hist_green, color='green', label='Green Channel')\n",
    "plt.plot(hist_red, color='red', label='Red Channel')\n",
    "plt.title('Histogramme Couleur avec Masque')\n",
    "plt.xlabel('Intensité des pixels')\n",
    "plt.ylabel('Nombre de pixels')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f4d3f-a823-46c2-bdb9-ce036bb9c3b2",
   "metadata": {},
   "source": [
    "### 10-1-1-3 Conversion en niveau de gris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375d162-0183-4539-bdad-261b84975416",
   "metadata": {},
   "source": [
    "L'image peut être convertie en niveaux de gris avec **cvtColor**, c'est une étape préalable necessaire à de nombreux traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6f6bb-a180-45f9-8eab-fb3613ec6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \n",
    "cv2.imshow(\"Niveaux de gris\",img_gray )\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579e9a4-08cd-4bc5-b675-60df22bf5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimensions : ',img_gray.shape)\n",
    "# Trouver les valeurs minimale et maximale avec numpy\n",
    "min_val = np.min(img_gray)\n",
    "max_val = np.max(img_gray)\n",
    "print('valeur mini pixel : ',min_val,'  valeur maxi pixel : ', max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdde33-7a71-469f-a428-6cdc6bba1be0",
   "metadata": {},
   "source": [
    "### 10-1-1-4 Sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca871704-b74a-4f1c-86f9-f8bc0e7e77b3",
   "metadata": {},
   "source": [
    "Il est possible de sauvegarder l'image après modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba577bd-7197-43ba-9dd0-d2357f86c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('fichiers_cours/traitement_image/fichiers_crees/nouvelle_image.jpg', img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defd110-e7cf-4d94-950f-efabf092d9db",
   "metadata": {},
   "source": [
    "## 10-1-2 Coordonnées et figures géométriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10119b-c13e-4994-b021-e76acf2bb751",
   "metadata": {},
   "source": [
    "On peut ajouter des figures géométriques sur l'image en précisant les coordonnées, les propriétés du trait..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1c70f-74d4-40d7-b79a-a6e6748b0933",
   "metadata": {},
   "source": [
    "### 10-1-2-1 Ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7836f3a-8095-4454-ad39-8af730706411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les points de début et de fin de la ligne\n",
    "start_point = (650, 200)\n",
    "end_point = (550, 50)\n",
    "\n",
    "# Définir la couleur de la ligne en format BGR (bleu, vert, rouge)\n",
    "color = (0, 0, 255)  # Rouge\n",
    "\n",
    "# Définir l'épaisseur de la ligne\n",
    "thickness = 3\n",
    "\n",
    "# Utiliser la fonction line pour dessiner la ligne\n",
    "img_draw = cv2.line(img, start_point, end_point, color, thickness)\n",
    "\n",
    "# Afficher l'image avec la ligne dessinée\n",
    "cv2.imshow(\"Ligne Rouge\", img_draw)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98332577-07d5-4b15-94ab-2a98e8285846",
   "metadata": {},
   "source": [
    "### 10-1-2-2 Cercle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3596f-7c5d-40dd-89db-40b5eaff00d6",
   "metadata": {},
   "source": [
    "Un cercle, à partir des coordonnées de son centre et de son rayon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0db33-aadc-4a20-973e-09537aba6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dessiner un cercle rouge\n",
    "center_coordinates = (100, 100)\n",
    "radius = 50\n",
    "color = (0, 0, 255)  # Rouge en format BGR\n",
    "thickness = 3\n",
    "\n",
    "# Utiliser la fonction circle pour dessiner le cercle\n",
    "img_draw = cv2.circle(img, center_coordinates, radius, color, thickness)\n",
    "\n",
    "cv2.imshow(\"Figures\",img_draw )\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9edde-3beb-4570-92ab-f9024a834fde",
   "metadata": {},
   "source": [
    "Si on veut une figure pleine, on met le thickness à -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51f881-346f-45f1-8f8f-2582dfa8fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness = -1\n",
    "# Utiliser la fonction circle pour dessiner le cercle\n",
    "img_draw = cv2.circle(img, center_coordinates, radius, color, thickness)\n",
    "\n",
    "cv2.imshow(\"Figures\",img_draw )\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7789c-e8e0-4f56-9659-1f8edb887ac2",
   "metadata": {},
   "source": [
    "### 10-1-2-3 Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada93ebf-610f-425c-8f22-8df6b7787deb",
   "metadata": {},
   "source": [
    "Rectangle à partir de 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33219ccc-afd8-410d-9a9f-c90c6f0eee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dessiner un rectangle vert\n",
    "start_point = (350, 250)\n",
    "end_point = (500, 550)\n",
    "color = (0, 255, 0)  # Vert en format BGR\n",
    "thickness = 3\n",
    "\n",
    "# Utiliser la fonction rectangle pour dessiner le rectangle\n",
    "img_draw = cv2.rectangle(img, start_point, end_point, color,thickness)\n",
    "\n",
    "cv2.imshow(\"Figures\",img_draw )\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d52dd5-1292-4fde-8866-eaccfa571f6e",
   "metadata": {},
   "source": [
    "Visualisons les points start point et end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dccbc9-59e1-45de-951e-a386a1010a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0, 0, 255)\n",
    "radius = 8\n",
    "thickness =-1\n",
    "\n",
    "img_draw = cv2.circle(img_draw, start_point, radius, color, thickness)\n",
    "img_draw = cv2.circle(img_draw, end_point, radius, color, thickness)\n",
    "\n",
    "cv2.imshow(\"Figures\",img_draw )\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602ef53-a9ce-42bf-81a0-3de4ea63b942",
   "metadata": {},
   "source": [
    "### 10-1-2-4 Polygone "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f316c7-810a-4d10-b7b4-6747c5f21cad",
   "metadata": {},
   "source": [
    "Polygone à partir des coordonnées de ses sommets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d7dee-559c-4504-968c-674208cceeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Définir les angles d'un pentagone\n",
    "pentagon_points = np.array([\n",
    "    [0, 300],  # Point 1\n",
    "    [100, 200],  # Point 2\n",
    "    [200, 200],  # Point 3\n",
    "    [300, 300],  # Point 4\n",
    "    [150, 400],  # Point 5\n",
    "], np.int32)\n",
    "# Reshape pour s'assurer que l'array est dans le bon format\n",
    "pentagon_points = pentagon_points.reshape((-1, 1, 2)) # A PRECISER\n",
    "# Dessiner le pentagone (rouge) sur l'image\n",
    "color = (250, 50, 50)  # Couleur en format BGR\n",
    "thickness = 3\n",
    "\n",
    "# Utiliser la fonction polylines pour dessiner le pentagone\n",
    "img_pentagon = cv2.polylines(img, [pentagon_points], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Figures\",img_pentagon)\n",
    "k = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8129ed-c128-4b0e-baad-f9f0cae24572",
   "metadata": {},
   "source": [
    "## 10-1-3 Filtres et seuillages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0d1a6-3458-4ab7-88aa-f62e14d8d5a5",
   "metadata": {},
   "source": [
    "Le seuillage en traitement d'image est essentiel pour simplifier et segmenter les images en distinguant les objets d'intérêt du fond. On facilite ainsi l'analyse et la détection.\n",
    "La fonction threshold ne peut être appliqué qu'à une image en niveaux de gris, commençons par une conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187aa82-5d77-4027-bed0-1c770c0d7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'image en couleur\n",
    "img = cv2.imread(\"fichiers_cours/traitement_image/formes.png\")\n",
    "\n",
    "# Convertir l'image en niveaux de gris\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Afficher l'image en couleur sur le premier sous-plot\n",
    "ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title('Image en couleur')\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Afficher l'image en niveaux de gris sur le deuxième sous-plot\n",
    "ax[1].imshow(img_gray, cmap='gray')\n",
    "ax[1].set_title('Image en niveaux de gris')\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f83a2-b67f-4a43-93b9-239a5e6e2bdf",
   "metadata": {},
   "source": [
    "#### 10-1-3-1 Seuillage Binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee9f7d-9674-4516-b86e-3fe81b60c41c",
   "metadata": {},
   "source": [
    "Appliquons maintenant un seuillage binaire à l'image, en choisissant un niveau gris au dessous duquel on mettra la valeur des pixels à zero (noir), c'est le seuil,  et un niveau de fond, c'est la valeur qu'on donnera à tous pixels au dessus du seuil (généralement 255 blanc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2a22c-ddf2-4ef2-a78a-755a0c3cd26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil = 220\n",
    "fond = 255\n",
    "\n",
    "ret1,threshold=cv2.threshold(img_gray,seuil,fond,cv2.THRESH_BINARY) \n",
    "print('reth1 = ',ret1)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(cv2.cvtColor(threshold, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title('Image en couleur')\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35b8f4-3195-48a2-96cf-58436941caf0",
   "metadata": {},
   "source": [
    "En jouant sur fond, on peut afficher un fond gris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d30e8-cb6c-43fd-a6f4-69604aed093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil = 220\n",
    "fond = 100\n",
    "\n",
    "ret1,threshold=cv2.threshold(img_gray,seuil,fond,cv2.THRESH_BINARY) \n",
    "print('reth1 = ',ret1)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(cv2.cvtColor(threshold, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title('Image en fond gris')\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f94e2-8a33-48e5-9b7f-73f86b2b0a23",
   "metadata": {},
   "source": [
    "Sur notre figure, testons différents niveaux de seuil, attention, ici pour des question d'affichage, nous utiliserons un THRESH_BINARY_INV qui mettra le fond à zero et les objets à 255 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9bdac-c349-4b2f-a7ee-459be267c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "seuils = [80,100,150,220]\n",
    "# Créer une grille de sous-graphiques (3 lignes, 2 colonnes)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "# Afficher l'image originale (img)\n",
    "axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Image originale (img)')\n",
    "\n",
    "# Afficher l'image en niveaux de gris (img_gray)\n",
    "axes[0, 1].imshow(img_gray, cmap='gray')\n",
    "axes[0, 1].set_title('Image en niveaux de gris (img_gray)')\n",
    "\n",
    "# Boucle sur les seuils et afficher les images seuillées\n",
    "for i, s in enumerate(seuils,start=1): # start = 1 car on a deja des subplots en 0 et 1\n",
    "    ret1, threshold = cv2.threshold(img_gray, s, 255, cv2.THRESH_BINARY_INV)\n",
    "    row = (i + 1) // 2\n",
    "    col = (i + 1) % 2\n",
    "    axes[row, col].imshow(threshold, cmap='gray')\n",
    "    axes[row, col].set_title(f'Seuil = {s}')\n",
    "\n",
    "# Ajuster l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3ef32-4e9e-4fbb-95f9-e2f6b636a9df",
   "metadata": {},
   "source": [
    "On a donc des niveaux de gris differents pour les 3 formes geometriques.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac87aea-8ede-4ecc-947b-48885cb85f85",
   "metadata": {},
   "source": [
    "#### 10-1-3-2 Seuillage de Masque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f75b4-9db6-4b55-948c-7e7dc4c53454",
   "metadata": {},
   "source": [
    "On peut également creer un masque pour selectionner une plage de valeurs de pixels. Attention, dans la partie 10-1-1-2 le mask concernait une zone de l'image, c'est à dire des coordonnées de pixels. Ici c'est un masque sur des intensités, on choisit une plage d'intensités. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e4d29-e97e-4b0f-bae1-e69fa7b906df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les seuils haut et bas\n",
    "seuil_bas = 200\n",
    "seuil_haut = 250\n",
    "\n",
    "# Appliquer le seuillage\n",
    "mask = cv2.inRange(img_gray, seuil_bas, seuil_haut)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(mask,cmap='gray')#, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title('Seuillage');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aba0ed-515c-40dc-bb81-3e5383aa78eb",
   "metadata": {},
   "source": [
    "#### 10-1-3-3 Seuillage Adaptatif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59524c-af2a-4c2f-b12e-bdf97bcf7c60",
   "metadata": {},
   "source": [
    "Les seuils adaptatifs permettent d'adapter le seuil en fonction de la zone de l'image, c'est à dire en fonction des pixels voisins. Ceci permet de travailler sur des images avec des eclairages irréguliers.\n",
    "La documentation OpenCv fournit l'exemple suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277b0d5-ca18-4cf8-afd6-2c764b8c2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "img = cv.imread('fichiers_cours/traitement_image/sudoku.png', cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "img = cv.medianBlur(img,5)\n",
    "\n",
    "ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "titles = ['Original Image', 'Binary Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5e9e8-0359-4692-978f-9f65418dcca3",
   "metadata": {},
   "source": [
    "D'autres types de seuillages sont disponibles dans la [doc OpenCV](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ee9d7-dc55-4262-a59d-7ed5b3bb312c",
   "metadata": {},
   "source": [
    "#### 10-1-3-4 Seuillage sur des couleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01c978-1193-434b-aaf0-c90c5a909d09",
   "metadata": {},
   "source": [
    "L'espace des couleurs HSV (Hue, Saturation, Luminance) est un modèle de représentation des couleurs qui sépare les informations de teinte, de saturation et de luminance d'une couleur. Il est souvent utilisé dans le domaine du traitement d'images et de la vision par ordinateur pour faciliter la manipulation et la détection des couleurs. \n",
    "\n",
    "- Teinte (H - Hue) :\n",
    "La teinte représente la couleur elle-même. Elle est mesurée en degrés sur le cercle chromatique, où chaque degré correspond à une couleur spécifique.\n",
    "La plage de valeurs de la teinte dans l'espace HSV est généralement de 0 à 360 degrés, mais dans OpenCV, elle est souvent normalisée à une plage de 0 à 179 pour tenir compte des limitations de stockage.\n",
    "\n",
    "- Saturation (S - Saturation) :\n",
    "La saturation mesure l'intensité de la couleur. Une saturation élevée signifie des couleurs vives et éclatantes, tandis qu'une saturation faible conduit à des couleurs plus ternes et proches du gris.\n",
    "La plage de valeurs de la saturation dans l'espace HSV est de 0 à 255 dans OpenCV.\n",
    "\n",
    "- Luminance (V - Value) ou Brillance (B - Brightness) :\n",
    "La luminance représente la luminosité ou la brillance de la couleur. Une luminance élevée signifie une couleur plus claire, tandis qu'une luminance faible donne une couleur plus sombre.\n",
    "La plage de valeurs de la luminance dans l'espace HSV est de 0 à 255 dans OpenCV.\n",
    "\n",
    "On peut visualiser cette representation de la manière suivante:  \n",
    "\n",
    "![hsv](fichiers_cours/traitement_image/hsv1.jpg)\n",
    "\n",
    "\n",
    "Pour trouver les valeurs, on peut s'aider d'une carte de couleur ou d'un [calculateur de couleurs](https://www.selecolor.com/en/hsv-color-picker/) Attention H est limité à 180° sur OpenCv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54059bc-7283-4ade-ae05-b02da39fbe47",
   "metadata": {},
   "source": [
    "Appliquons cette detection à nos tournesols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c5d78-c33d-4965-9297-f7b6debaaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'image\n",
    "img = cv2.imread(\"fichiers_cours/traitement_image/tournesols.jpg\")\n",
    "\n",
    "# Convertir l'image BGR en image HSV\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Définir les plages de couleur pour le jaune dans l'espace HSV (on peut s'aider de\n",
    "lower_yellow = np.array([20, 100, 100])\n",
    "upper_yellow = np.array([30, 255, 255])\n",
    "\n",
    "# Créer un masque en utilisant les plages spécifiées\n",
    "mask = cv2.inRange(img_hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title('Image en couleur')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(mask, cmap='gray')\n",
    "ax[1].set_title('Detection couleurs')\n",
    "ax[1].axis('off');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d909d-b831-4ef2-abc4-cb2d47b83e4e",
   "metadata": {},
   "source": [
    "## 10-1-4 Détection de contours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05173111-4465-40ee-affc-42d19e7ea0fe",
   "metadata": {},
   "source": [
    "#### 10-1-4-1 Filtre de Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca2621-0e50-4e5a-a51e-258009befd1a",
   "metadata": {},
   "source": [
    "Pour détecter les contours d'un objet, On peut appliquer des filtres comme un filtre de Canny, c'est un algorithme détecte les contours en trouvant les changements d'intensité dans une image. Il inclut plusieurs étapes, y compris la réduction du bruit, la recherche de gradients, et la suppression des pixels non-maxima.\n",
    "\n",
    "https://docs.opencv.org/3.4/da/d22/tutorial_py_canny.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b32ab-f906-4dfe-bb36-3ab37be83178",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"fichiers_cours/traitement_image/bonbons.png\")\n",
    "\n",
    "seuil_bas = 150 \n",
    "seuil_haut = 550\n",
    "# img_blur = cv2.blur(img_gray,(3,3))\n",
    "\n",
    "edges = cv2.Canny(img, seuil_haut, seuil_bas) \n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title('Origine')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(edges, cmap='gray')\n",
    "ax[1].set_title('Canny')\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dcf95-1652-4513-96e1-eacf35502908",
   "metadata": {},
   "source": [
    "#### 10-1-4-2 Transformée de Hough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79939b27-6c97-4d1d-a7ff-ca75556ea39c",
   "metadata": {},
   "source": [
    "La tranformée de Hough permet de reconnaitre des formes comme des cercles ou des lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0879d7a-3125-4615-9b31-b837568dcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Paramètres pour la transformée de Hough\n",
    "resolution_accumulator = 1\n",
    "min_distance_between_circles = 10\n",
    "threshold_canny = 100\n",
    "threshold_accumulator = 30\n",
    "min_radius = 1\n",
    "max_radius = 30\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Appliquer la transformée de Hough pour détecter les cercles\n",
    "circles = cv2.HoughCircles(\n",
    "    img_gray,\n",
    "    cv2.HOUGH_GRADIENT,\n",
    "    dp=resolution_accumulator,\n",
    "    minDist=min_distance_between_circles,\n",
    "    param1=threshold_canny,\n",
    "    param2=threshold_accumulator,\n",
    "    minRadius=min_radius,\n",
    "    maxRadius=max_radius\n",
    ")\n",
    "\n",
    "# Convertir les coordonnées en entiers\n",
    "circles = circles.astype(np.uint16)\n",
    "\n",
    "# Dessiner les cercles détectés sur l'image\n",
    "for i in circles[0, :]:\n",
    "    cv2.circle(img, (i[0], i[1]), i[2], (255, 0, 255), 3)\n",
    "\n",
    "# Afficher l'image avec les cercles détectés\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(img, cmap='gray')\n",
    "ax.set_title('Cercles détectés')\n",
    "\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.title('Cercles détectés')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9394ae5-2363-409a-a58a-065cbb435180",
   "metadata": {},
   "source": [
    "cv2.HoughCircles renvoit un tableau numpy de la forme  [[[x1, y1, r1], [x2, y2, r2], ..., [xn, yn, rn]]] avec x et y coordonnées du centre et r rayon.\n",
    "\n",
    "On peut jouer sur les paramètres:  \n",
    "\n",
    "- resolution_accumulator :  Représente la résolution de l'accumulateur en pixels. Une valeur de 1 signifie que l'accumulateur a la même résolution que l'image d'entrée. Une résolution plus élevée peut accélérer le processus de détection, mais elle peut également être moins précise.\n",
    "\n",
    "- param1 : Seuil pour le détecteur de gradient. Il s'agit du seuil supérieur utilisé par le détecteur de contours de Canny, qui est intégré dans le processus de détection de cercles.  \n",
    "Une valeur plus élevée de param1 signifie que seuls les contours plus forts sont considérés comme des candidats pour la détection de cercles.  \n",
    "\n",
    "\n",
    "- param2 : Seuil pour  l'accumulateur de Hough.Une valeur plus petite de param2 signifie que moins de candidats potentiels (cercles) seront considérés comme valides. Cela aide à éliminer les cercles faux.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7283b4-efec-48c7-8d03-64b1f7aa595e",
   "metadata": {},
   "source": [
    "Detection de lignes avec tranformée de Hough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2022c57-ee2d-402e-9e5c-6e4aaa226e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img = cv2.imread('fichiers_cours/traitement_image/sudoku.png')\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Appliquer un flou pour réduire le bruit et faciliter la détection des contours\n",
    "blurred = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
    "\n",
    "# Détection des contours avec Canny\n",
    "edges = cv2.Canny(blurred, 50, 200, None, 3)\n",
    "\n",
    "# Appliquer la transformée de Hough pour détecter les lignes\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, 150, None, 0, 0)\n",
    "\n",
    "# Dessiner les lignes détectées sur l'image originale\n",
    "\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        rho, theta = line[0]\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        x1 = int(x0 + 1000 * (-b))\n",
    "        y1 = int(y0 + 1000 * (a))\n",
    "        x2 = int(x0 - 1000 * (-b))\n",
    "        y2 = int(y0 - 1000 * (a))\n",
    "        cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img_gray,cmap = 'gray')\n",
    "ax[1].imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700b3e8-75bd-4d4f-b799-3a366af584e4",
   "metadata": {},
   "source": [
    "#### 10-1-4-3 Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc9343-11e2-4ea2-a559-333baa987203",
   "metadata": {},
   "source": [
    "La fonction **findContours** est utilisée pour trouver et extraire les contours dans une image. Elle prend en entrée une image binaire (par exemple, résultat d'un seuillage) et retourne une liste de contours.  \n",
    "Contrairement à un filtre de canny qui retournera une image, la fonction **findcontours** retournera une liste de coordonnées de points définissant les contours. Ceci sera très utile pour extraire des mesures de dimension par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db9792-640c-476f-be2c-8b58823fface",
   "metadata": {},
   "source": [
    "Il faut commencer par seuiller l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077831a-e795-43ce-bfda-fbbde122fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"fichiers_cours/traitement_image/formes.png\")\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret1,threshold=cv2.threshold(img_gray,250,255,cv2.THRESH_BINARY)  \n",
    "plt.imshow(threshold,cmap = 'gray')\n",
    "plt.show()\n",
    "print(ret1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c59df-6e0a-46c9-aa4a-6473315e2a2f",
   "metadata": {},
   "source": [
    "Puis on détecte les contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f21be-8135-4ba0-b32d-a51def324131",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9785f-670d-4d06-bd5d-06a91dd0c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(contours),' contours trouves')\n",
    "print('Le contour n°1 comporte', contours[1].shape[0],'points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b51cc-4c31-40ea-97a5-467f6059bc35",
   "metadata": {},
   "source": [
    "Affichons un des contours détectés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bbcfe1-9ccb-4b30-8dad-cb82c38b777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenté pour la présentation du notebook\n",
    "print(contours[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762af932-5946-4486-85cb-651216aa5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensions du tableau hierachy = ',hierarchy.shape)\n",
    "print(hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78288d8-c186-4262-854e-fff475d5574b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224030f-dfec-4ddf-82ef-697451b40e4d",
   "metadata": {},
   "source": [
    "- threshold: L'image seuillée à partir de laquelle les contours seront extraits.\n",
    "\n",
    "- cv2.RETR_TREE: Le mode de récupération de contours. Il spécifie la manière dont les contours sont organisés dans la hiérarchie. cv2.RETR_TREE crée une structure de hiérarchie complète et reconstruit tous les contours.\n",
    "\n",
    "- cv2.CHAIN_APPROX_SIMPLE: La méthode d'approximation des contours. Elle spécifie comment les contours doivent être stockés après l'approximation. cv2.CHAIN_APPROX_SIMPLE compresse les segments laisse seulement leurs points finaux.\n",
    "\n",
    "La fonction retourne:\n",
    "\n",
    "- contours: Un tuple contenant tous les contours trouvés dans l'image seuillée. Chaque contour est représenté sous forme de tableau numpy 2D contenant les coordonnées (x, y) des points constituant le contour.\n",
    "\n",
    "- hierarchy: Une structure de données représentant la hiérarchie entre les contours. Elle est souvent utilisée pour identifier les contours internes et externes. Dans ce cas, elle n'est pas utilisée directement dans le code que vous avez fourni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f096a-6126-4166-a453-54b5454fe4c1",
   "metadata": {},
   "source": [
    "Le tableau hierarchy contient un tableau de 4 valeurs par contour.   \n",
    "Ces 4 valeurs représentent:   \n",
    "- Next: Index du contour suivant sur le même niveau hiérarchique.\n",
    "- Previous: Index du contour précédent sur le même niveau hiérarchique.\n",
    "- First_Child: Index du premier contour enfant.\n",
    "- Parent: Index du contour parent.\n",
    "      \n",
    "Par exemple la premiere ligne [-1, -1, 1, -1] indique:\n",
    "- Next (-1): Il n'y a pas de contour suivant sur le même niveau, donc la valeur est -1.\n",
    "- Previous (-1): Il n'y a pas de contour précédent sur le même niveau, donc la valeur est -1.\n",
    "- First_Child (1): Il a un contour enfant, et le premier contour enfant a l'index 1.\n",
    "- Parent (-1): Il n'a pas de contour parent.\n",
    "C'est le contours qui contient tous les autres.   \n",
    "La ligne [ 4  2 -1  0] indique:\n",
    "- Next (4): Il y a un contour suivant sur le même niveau, et son indice est 4.\n",
    "- Previous (2): Il y a un contour précédent sur le même niveau, et son indice est 2.\n",
    "- First_Child (-1): Il n'y a pas de contour enfant, donc la valeur est -1.\n",
    "- Parent (0): Il a un contour parent, et son indice est 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08cb64-5540-4f77-9bb3-fb9cd31efc12",
   "metadata": {},
   "source": [
    "Traçons les contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef933f-3cbf-475e-9cca-508a0f770d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cnt in contours:\n",
    "    cv2.drawContours(img, [cnt], -1, (255, 0, 0), 5)\n",
    "\n",
    "print(len(contours),'contours detectes')\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b7c7b-1676-450f-ac6a-a960d3601377",
   "metadata": {},
   "source": [
    "Ajoutons les numeros de chaque contour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198d7ff-c90f-4c03-9516-4bf5bcfa1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cnt in enumerate(contours):\n",
    "    cv2.drawContours(img, [cnt], -1, (255, 0, 0), 5)\n",
    "    x, y = cnt[0][0]# x et y representent le premier point de chaque contour\n",
    "\n",
    "    cv2.putText(img, str(i), (x+5,y+30), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2) # +5 et +30 pour que le zero soit dans l'image\n",
    "\n",
    "print(len(contours), 'contours détectés')\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773590d-ef11-4d78-ab62-41d79734176f",
   "metadata": {},
   "source": [
    "On peut englober les formes détéctées dans des rectangles. Ceci nous permettra de réaliser des mesures. Détectons les rectangles et plaçons un point vert au centre de chacun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df932846-5010-4ed6-851c-ac987adf1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"fichiers_cours/traitement_image/formes.png\")\n",
    "for cnt in contours:\n",
    "    # Obtenir le rectangle englobant orienté\n",
    "    rect = cv2.minAreaRect(cnt)\n",
    "    print(rect)\n",
    "    # Décomposition du tuple rect[0] pour obtenir les coordonnées x et y\n",
    "    coord0 = (int(rect[0][0]), int(rect[0][1]))\n",
    "    cv2.circle(img, coord0, 5, (0, 255, 0), -1) #point vert au centre de chaque rectangle\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4752cc8-1607-4cef-9449-226940bbe405",
   "metadata": {},
   "source": [
    "rect[0] : coordonnées du centre du rectangle.  \n",
    "rect[1] : dimensions du rectangle (largeur, hauteur).  \n",
    "rect[2] : l'angle de rotation du rectangle.\n",
    "\n",
    "On peut convertir ces valeurs avec cv2.boxPoints():  \n",
    "<pre>box = cv2.boxPoints(rect)</pre>\n",
    "Box contient les coordonnées des 4 angles du rectangle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d884bc4-0dc5-4fda-9fb7-3b603e8ea377",
   "metadata": {},
   "source": [
    "Utilisons les coordonnées récupérées par rect pour tracer les rectangles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78a558-5056-4d40-95d4-e64c9b74f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"fichiers_cours/traitement_image/formes.png\")\n",
    "for cnt in contours:\n",
    "    # Obtenir le rectangle englobant orienté\n",
    "    rect = cv2.minAreaRect(cnt)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.round(box).astype(int)\n",
    "    cv2.drawContours(img, [box], 0, (0, 255, 0), 2)  # Dessiner le rectangle en rouge\n",
    "    coord0 = tuple(map(int,rect[0]))\n",
    "    cv2.circle(img, coord0, 5, (0, 255, 0), -1)\n",
    "\n",
    "    # Point rouge à chaque coin du rectangle\n",
    "    for i in range(len(box)):\n",
    "        cv2.circle(img, tuple(box[i]), 5, (255, 0, 0), -1)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7244e73-c0cb-458c-a284-02496c482e42",
   "metadata": {},
   "source": [
    "Calculons les rapports de convexité  en divisant l'aire du contour d'origine par l'aire du polygone convexe. Cela permet d'évaluer à quel point le contour est convexe. Plus le rapport est proche de 1, plus le contour est convexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde10df2-7d04-4320-bc0b-a15ef12d194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"fichiers_cours/traitement_image/formes.png\")\n",
    "import cv2\n",
    "for cnt in contours:\n",
    "    # Dessiner les contours originaux (Rouge)\n",
    "    cv2.drawContours(img, [cnt], -1, (255, 0, 0), 3)\n",
    "\n",
    "    # Calculer le polygone convexe\n",
    "    hull = cv2.convexHull(cnt)\n",
    "\n",
    "    # Calculer la surface du polygone convexe et la surface du contour d'origine\n",
    "    area_hull = cv2.contourArea(hull)\n",
    "    area_contour = cv2.contourArea(cnt)\n",
    "\n",
    "    # Calculer le ratio de convexité\n",
    "    ratio_convexite = area_contour / area_hull\n",
    "    print(ratio_convexite)\n",
    "                   \n",
    "    # Dessiner le polygone convexe (vert)\n",
    "    cv2.drawContours(img, [hull], -1, (0, 255, 0), 3)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237d48a-a9c2-48a8-a756-c19ff866548c",
   "metadata": {},
   "source": [
    "Dans le cas des formes convexes, contour et hull sont confondus et on a un ration de 1, par contre l'étoile qui est concave a un ratio inferieur à 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80167094-ba04-458a-812f-8478949d271f",
   "metadata": {},
   "source": [
    "### 10-1-4-4 Préparation à la détection de contours: Filtres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3dc4f1-6771-4308-8b59-bb354af0e533",
   "metadata": {},
   "source": [
    "Si on a un fond bruité, la détection devient compliquée, on peut avoir besoin d'appliquer des filtres à l'image, utilisons ici **cv2.blur** qui fera une moyenne des pixels par zone et apportera un flou à l'image. D'autre filtres peuvent être utilisés (GaussianBlur MedianBlur...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f280636-79c8-4899-b586-b2f1e00a7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"fichiers_cours/traitement_image/moon.jpg\")\n",
    "\n",
    "# Convertir en niveaux de gris\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Appliquer le flou\n",
    "img_blur = cv2.blur(img_gray, (11, 11))\n",
    "\n",
    "# Appliquer le seuillage avant le flou\n",
    "ret1, threshold1 = cv2.threshold(img_gray, 140, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Appliquer le seuillage après le flou\n",
    "ret2, threshold2 = cv2.threshold(img_blur, 140, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Créer une figure avec des sous-graphiques (2 lignes, 2 colonnes)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Afficher l'image originale\n",
    "axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Image Originale')\n",
    "\n",
    "# Afficher l'image après le flou\n",
    "axes[0, 1].imshow(img_blur, cmap='gray')\n",
    "axes[0, 1].set_title('Image après le Flou')\n",
    "\n",
    "# Afficher l'image seuillée avant le flou\n",
    "axes[1, 0].imshow(threshold1, cmap='gray')\n",
    "axes[1, 0].set_title('Image Seuillée (Avant le Flou)')\n",
    "\n",
    "# Afficher l'image seuillée après le flou\n",
    "axes[1, 1].imshow(threshold2, cmap='gray')\n",
    "axes[1, 1].set_title('Image Seuillée (Après le Flou)')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fe967-f595-42d1-9283-4ee1046aa3db",
   "metadata": {},
   "source": [
    "Cette étape préliminaire permettra de \"simplifier\" l'image avant de faire la detection de contours. L'effet sera de ne détecter que les contours des objets principaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44791f-fb7d-4b05-a1a2-6d8ace70b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours1, hierarchy1 = cv2.findContours(threshold1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours2, hierarchy2 = cv2.findContours(threshold2, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Créer une figure avec des sous-graphiques (2 lignes, 2 colonnes)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Afficher l'image originale\n",
    "axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Image Originale')\n",
    "\n",
    "# Afficher les contours avant le flou\n",
    "img_contours1 = img.copy()\n",
    "for cnt in contours1:\n",
    "    cv2.drawContours(img_contours1, [cnt], -1, (0, 0, 255), -1)\n",
    "axes[0, 1].imshow(cv2.cvtColor(img_contours1, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 1].set_title('Contours (Avant le Flou)')\n",
    "\n",
    "# Afficher l'image après le flou\n",
    "axes[1, 0].imshow(img_blur, cmap='gray')\n",
    "axes[1, 0].set_title('Image après le Flou')\n",
    "\n",
    "# Afficher les contours après le flou\n",
    "img_contours2 = img.copy()\n",
    "for cnt in contours2:\n",
    "    cv2.drawContours(img_contours2, [cnt], -1, (0, 0, 255), -1)\n",
    "axes[1, 1].imshow(cv2.cvtColor(img_contours2, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].set_title('Contours (Après le Flou)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Nombre de contours (Avant le Flou):\", len(contours1))\n",
    "print(\"Nombre de contours (Après le Flou):\", len(contours2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58362b-2fa0-4f6a-92ae-377c018e6c29",
   "metadata": {},
   "source": [
    "# 10-2 La bibliothèque Scikit Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad22e69-9349-4fc5-b21e-44d179ad3d3d",
   "metadata": {},
   "source": [
    "Scikit-image est dédiée au traitement et à la manipulations d'images scientifiques alors que OpenCv est plus axé sur la vision par ordinateur. Cette bibliothèque offre des outils puissants pour le prétraitement, la segmentation et l'extraction de caractéristiques. Les algorithmes utilisés sont souvent mieux documentés donc plus adaptés à un usage scientifique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b7539-f0f4-4d0c-a6b0-e6e0efb0ba45",
   "metadata": {},
   "source": [
    "## 10-2-1 Opérations morphologiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2621555-a50f-49b4-ab49-2c8ecabfb8da",
   "metadata": {},
   "source": [
    "Les opérations morphologiques sont des techniques de traitement d'images visant à manipuler la forme et la structure des objets. Elles incluent l'érosion, qui réduit la taille des objets en retirant leurs bords, et la dilatation, qui les agrandit en ajoutant des pixels autour. Ces opérations, utilisant des éléments structurants comme des disques ou des carrés, sont efficaces pour traiter des problèmes tels que la suppression de bruit, la séparation d'objets connectés, et la détection de contours. Les opérations morphologiques jouent un rôle crucial dans la préparation d'images pour l'analyse et la reconnaissance d'objets dans le domaine du traitement d'images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646417a9-4112-44f5-bda0-a6bc67ca7f24",
   "metadata": {},
   "source": [
    "Ouvrons d'abord une image avec scikit image.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5386f9-cef7-4185-b924-bbcbcc741890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import skimage.io as io\n",
    "\n",
    "# Load the image\n",
    "image_path = \"fichiers_cours/traitement_image/j.png\"\n",
    "image = io.imread(image_path)\n",
    "#convertir l'image en niveaux de gris\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the image\n",
    "io.imshow(image)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6f0b9-b4a6-4747-835e-65f3575a9d1c",
   "metadata": {},
   "source": [
    "### 10-2-1-1 Erosion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b9997-049b-4cff-9314-1d233f67af26",
   "metadata": {},
   "source": [
    "Un élément structurant est une petite forme géométrique, telle qu'un carré ,utilisé dans les opérations morphologiques sur les images, ici un disque de rayon 2 pixels. Il agit comme un motif pour définir le comportement de l'opération morphologique, comme l'érosion ou la dilatation. L'élément structurant \"balaye\" l'image pixel par pixel, déterminant comment les pixels voisins influencent le résultat de l'opération."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d642fca-2a79-4286-a368-f8b8262e66ba",
   "metadata": {},
   "source": [
    "L'érosion va globalement afiner les traits, elle peut être utilisée spécifiquement pour la suppression de bruit ou pour la séparation d'objets connectés sur une image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfca74-e346-4fc0-bd7d-c8f0a6ecbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import erosion, disk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "footprint = disk(2)  # Element structurant\n",
    "eroded_image = erosion(image, footprint)\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Original  Image')\n",
    "ax[1].imshow(eroded_image, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Eroded  Image')\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c6261-c91c-4427-916a-2ff96af6fce1",
   "metadata": {},
   "source": [
    "### 10-2-2 Dilatation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67008cbe-1b78-42c0-91fa-607a54954ca8",
   "metadata": {},
   "source": [
    "La dilatation va globalement épaissir les traits, elle peut ête utilisé en préparation de la detection de contours ou pour boucher des trous dans un objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e923c-882c-4f20-ab32-3f45e8eebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import dilation\n",
    "\n",
    "# Apply dilation with a disk-shaped structuring element\n",
    "footprint_dilation = disk(6)  # You can adjust the size of the disk\n",
    "dilated_image = dilation(eroded_image, footprint_dilation)\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Original  Image')\n",
    "ax[1].imshow(dilated_image, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Dilated  Image')\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc7bbe-6681-4294-8203-fd22b8e7fb3f",
   "metadata": {},
   "source": [
    "### 10-2-3 Algorithme Watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c266317-84fd-453e-a7fb-e2a2ba1c3c37",
   "metadata": {},
   "source": [
    "L'algorithme Watershed (littéralement \"bassin versant\" en anglais) est utilisé pour la segmentation, c'est-à-dire la partition d'une image en régions ou objets distincts. Son nom est inspiré par l'analogie avec les bassins versants dans la topographie, où l'eau de pluie s'écoule naturellement vers les points bas formant des bassins.  \n",
    "\n",
    "\n",
    "- L'algorithme considère l'image comme un relief topographique où les niveaux de gris représentent l'altitude.\n",
    "- Les régions de l'image où l'intensité des niveaux de gris est élevée sont analogues à des pics de montagnes, et les points bas correspondent à des vallées.\n",
    "- On simule un remplissage d'eau à partir de ces points bas. L'eau commence à remplir les bassins versants jusqu'à ce que des bassins distincts se rejoignent.\n",
    "- Les lignes où l'eau se rejoint forment des lignes de partage des eaux, qui servent de frontières pour la segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecae1ff-dbc5-4163-bdbf-8f9e172f8a50",
   "metadata": {},
   "source": [
    "Utilisons l'exemple fourni dans la [documentation de Scikit Image](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html) pour l'illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7a089-b889-437a-b1bb-bfa50ab5891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlapping objects, Distances, Separated objects\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
    "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Overlapping objects')\n",
    "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Distances')\n",
    "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
    "ax[2].set_title('Separated objects')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f6077-dc1c-4849-9ddd-30d422eb5049",
   "metadata": {},
   "source": [
    "L'image de distances permet de visualiser la distance de chaque pixel au pixel de fond le plus proche. Les pics dans la transformation de distance sont des marqueurs potentiels qui peuvent être utilisés dans la segmentation watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bbe50-236a-4a8a-9796-435463440569",
   "metadata": {},
   "source": [
    "A partir de là, on peut détecter les contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a9318-f192-4071-b2df-8822f0d1ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir l'image en niveaux de gris en image uint8\n",
    "image_uint8 = image.astype(np.uint8) * 255\n",
    "\n",
    "# Trouver les contours du cercle 1\n",
    "contours_circle1, _ = cv2.findContours(mask_circle1.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Trouver les contours du cercle 2\n",
    "contours_circle2, _ = cv2.findContours(mask_circle2.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Créer une copie de l'image en couleur pour dessiner les contours\n",
    "image_with_contours_circle1 = cv2.cvtColor(image_uint8.copy(), cv2.COLOR_GRAY2BGR)\n",
    "image_with_contours_circle2 = cv2.cvtColor(image_uint8.copy(), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# Dessiner les contours sur les images en couleur\n",
    "image_with_contours_circle1 = cv2.drawContours(image_with_contours_circle1, contours_circle1, -1, (0, 255, 0), 2)\n",
    "image_with_contours_circle2 = cv2.drawContours(image_with_contours_circle2, contours_circle2, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Afficher les deux images avec les contours\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(image_with_contours_circle1)\n",
    "axes[0].set_title('Contours du cercle 1')\n",
    "\n",
    "axes[1].imshow(image_with_contours_circle2)\n",
    "axes[1].set_title('Contours du cercle 2')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd9ec5-fa4f-4217-aa8b-8a2c94cf4563",
   "metadata": {},
   "source": [
    "# 10-3 Traitement de videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afb914-bb99-4fc7-9dd3-9c52b613f04c",
   "metadata": {},
   "source": [
    "## 10-3-1 Lecture de vidéos avec OpenCv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c0c29-2c24-45d1-af74-f6a392e6a9c9",
   "metadata": {},
   "source": [
    "OpenCv peut ouvrir et lire des fichiers video. \n",
    "On ouvre le fichier avec cap = cv2.VideoCapture(chemin) pour ouvrir un fichier ou cap = cv2.VideoCapture(0) pour ouvir la webcam.  \n",
    "On lit ensuite les frame dans une boucle while.  \n",
    "Enfin, on prévoit la fermeture , ici avec la touche q et on libère les ressources video et éviter les problèmes de mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac653be-315f-4222-aa3b-7ab002346370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Chemin de la vidéo\n",
    "video_path = \"fichiers_cours/traitement_image/ma_video.mp4\"\n",
    "# Charger la vidéo\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Boucle pour lire chaque frame de la vidéo\n",
    "while True:\n",
    "    # Lire une frame\n",
    "    ret, frame = cap.read()\n",
    "    # Quitter la boucle si la frame n'est pas lue correctement\n",
    "    if not ret:\n",
    "        break \n",
    "    # Afficher la frame en niveaux de gris\n",
    "    cv2.imshow('Ma Video',frame)  \n",
    "    # Attendre la touche 'q' pour quitter la boucle\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Libérer la capture vidéo\n",
    "cap.release()\n",
    "# Fermer toutes les fenêtres\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc9b1b-01c8-4333-a172-caf9e87b8448",
   "metadata": {},
   "source": [
    "## 10-3-2 Traitement des images d'une vidéo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29764444-9011-4e4b-a11f-9c1ad31359dc",
   "metadata": {},
   "source": [
    "La quasi totalité des outils vus pour les image sera transposable pour le traitement des vidéos. \n",
    "Prenons l'exemple de la conversion en niveau de gris qui sera obtenue avec gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7d9fb-8e00-40bf-b075-37378461b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin de la vidéo\n",
    "video_path = \"fichiers_cours/traitement_image/ma_video.mp4\"\n",
    "# Charger la vidéo\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Boucle pour lire chaque frame de la vidéo\n",
    "while True:\n",
    "    # Lire une frame\n",
    "    ret, frame = cap.read()   \n",
    "    # Quitter la boucle si la frame n'est pas lue correctement\n",
    "    if not ret:\n",
    "        break   \n",
    "    # Convertir la frame en niveaux de gris\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)   \n",
    "    # Afficher la frame en niveaux de gris\n",
    "    cv2.imshow('Gray Video', gray_frame)  \n",
    "    # Attendre la touche 'q' pour quitter la boucle\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Libérer la capture vidéo\n",
    "cap.release()\n",
    "# Fermer toutes les fenêtres\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7f4b8-fe6e-4261-bd31-66220b50c45d",
   "metadata": {},
   "source": [
    "OpenCv donne également accès à des fonctionnalités s'appuyant sur l'intelligence artificielle, par exemple\n",
    "cv2.CascadeClassifier(cv2.data.haarcascades)  permet de charger des classificateurs en cascade pré-entraînés utilisés dans la détection d'objets. Les classificateurs en cascade sont des fichiers XML qui contiennent des informations sur la structure d'un modèle de détection d'objet spécifique, entraîné à détecter des caractéristiques visuelles telles que des visages, des yeux, des voitures, etc.\n",
    "\n",
    "Testons ici la détection de visages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826937ec-861b-4901-937a-b8855488b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Chemin de la vidéo\n",
    "video_path = \"fichiers_cours/traitement_image/ma_video.mp4\"\n",
    "# Charger la vidéo\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Charger le fichier de cascade pour la reconnaissance faciale\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "while True:\n",
    "    # Lire la frame de la vidéo\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  \n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Détecter les visages dans l'image\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    # Dessiner des rectangles autour des visages détectés\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    # Afficher la frame avec les visages détectés\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "    # Attendre que la touche 'q' soit pressée pour quitter\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Libérer la capture vidéo\n",
    "cap.release()\n",
    "# Fermer toutes les fenêtres\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
